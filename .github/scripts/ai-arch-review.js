#!/usr/bin/env node
/**
 * AI-powered architectural review using Claude API with principal-engineer agent
 *
 * This script analyzes recent code changes and reviews them against established
 * architectural patterns to identify:
 * - Code duplication across modules
 * - Architectural drift from documented patterns
 * - SOLID principles violations
 * - Security boundary violations
 * - Performance regressions
 * - Missing or outdated architectural documentation
 *
 * Usage:
 *   node .github/scripts/ai-arch-review.js [--days=7] [--significance-threshold=10] [--dry-run]
 *
 * Options:
 *   --days=N                    Number of days to look back for changes (default: 7)
 *   --significance-threshold=N  Minimum lines changed to be significant (default: 10)
 *   --dry-run                   Run review without creating GitHub issues
 *
 * Environment Variables:
 *   ANTHROPIC_API_KEY  Required: API key for Claude
 *   GITHUB_TOKEN       Required for creating issues (auto-provided in GitHub Actions)
 *
 * Outputs:
 *   - Architectural review report in JSON format
 *   - Creates GitHub issues for significant findings (unless --dry-run)
 *   - Recommends documentation updates
 */

import { execSync } from 'child_process';
import fs from 'fs';
import path from 'path';
import os from 'os';
import { fileURLToPath } from 'url';
import Anthropic from '@anthropic-ai/sdk';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const PROJECT_ROOT = path.resolve(__dirname, '../..');

// Constants
const MODEL = 'claude-sonnet-4-5-20250929';  // Claude Sonnet 4.5
const MAX_TOKENS = 8192;  // Larger token budget for architectural analysis

// Architecture documentation paths
const ARCHITECTURE_DOCS_PATHS = [
  'internal-docs/architecture/',
  'CLAUDE.md',
  'AGENTS.md',
  'internal-docs/facts.json',
];

// Significant change thresholds
const DEFAULT_SIGNIFICANCE_THRESHOLD = 50;  // Minimum lines changed (realistic for architectural review)

// Issue preamble
const ARCHITECTURE_ISSUE_PREAMBLE = `## üèóÔ∏è Architectural Review Context

This issue was generated by the AI architectural review system, which analyzes code changes weekly using a principal-engineer agent.

**For Agents Working on This Issue:**

Before addressing these architectural concerns, review:
- [ARCHITECTURE_QUICK_REFERENCE.md](internal-docs/architecture/ARCHITECTURE_QUICK_REFERENCE.md) - Quick lookup for all architectural guidance
- [ERROR_HANDLING.md](internal-docs/architecture/ERROR_HANDLING.md) - Error patterns and anti-patterns
- [SECURITY_MODEL.md](internal-docs/architecture/SECURITY_MODEL.md) - Security boundaries and threat model
- [PERFORMANCE.md](internal-docs/architecture/PERFORMANCE.md) - Performance targets and optimization patterns
- [principal-engineer agent](.claude/agents/principal-engineer.md) - Engineering philosophy and approach

**Key Principles:**
- **SOLID Principles**: Single Responsibility, Open/Closed, Liskov Substitution, Interface Segregation, Dependency Inversion
- **DRY (Don't Repeat Yourself)**: Eliminate duplication, but recognize some duplication is better than the wrong abstraction
- **YAGNI (You Aren't Gonna Need It)**: Build for today's requirements
- **KISS (Keep It Simple)**: Simplicity requires discipline

---`;

/**
 * Get recent code changes from git history with diffs
 *
 * @param {number} days - Number of days to look back
 * @param {number} significanceThreshold - Minimum lines changed to be significant
 * @returns {Object} Change summary with diffs
 */
function getSignificantChanges(days, significanceThreshold = DEFAULT_SIGNIFICANCE_THRESHOLD) {
  try {
    const since = `${days}.days.ago`;

    // Get commit log
    const commitLog = execSync(
      `git log --since="${since}" --pretty=format:"%h|%s|%an|%ar" --no-merges`,
      {
        cwd: PROJECT_ROOT,
        encoding: 'utf-8',
      }
    );

    const commits = commitLog
      .split('\n')
      .filter(Boolean)
      .map((line) => {
        const [hash, subject, author, date] = line.split('|');
        return { hash, subject, author, date };
      });

    // Get the oldest commit from the time range to use as base for diff
    let baseCommit = 'HEAD';
    if (commits.length > 0) {
      const oldestCommitHash = commits[commits.length - 1].hash;
      // Get the parent of the oldest commit in range
      try {
        baseCommit = execSync(`git rev-parse ${oldestCommitHash}^`, {
          cwd: PROJECT_ROOT,
          encoding: 'utf-8',
        }).trim();
      } catch (error) {
        // If no parent (first commit), use the commit itself
        baseCommit = oldestCommitHash;
      }
    }

    // Get changed files with line counts using proper git diff
    let changedFilesRaw = '';
    if (commits.length > 0) {
      changedFilesRaw = execSync(
        `git diff --numstat ${baseCommit}..HEAD`,
        {
          cwd: PROJECT_ROOT,
          encoding: 'utf-8',
        }
      );
    }

    const changedFiles = changedFilesRaw
      .split('\n')
      .filter(Boolean)
      .map((line) => {
        const [added, deleted, file] = line.split(/\s+/);
        const totalChanges = parseInt(added, 10) + parseInt(deleted, 10);
        return {
          file,
          added: parseInt(added, 10),
          deleted: parseInt(deleted, 10),
          totalChanges,
        };
      })
      .filter((f) => f.totalChanges >= significanceThreshold);

    // Categorize significant changes
    const backendChanges = changedFiles.filter((f) => f.file.startsWith('backend/src/'));
    const frontendChanges = changedFiles.filter((f) => f.file.startsWith('frontend/src/'));
    const testChanges = changedFiles.filter((f) => f.file.includes('/test'));

    // Get diffs for significant files (limit to top 30 most changed files)
    const significantFiles = changedFiles
      .sort((a, b) => b.totalChanges - a.totalChanges)
      .slice(0, 30);

    const diffs = [];
    for (const fileInfo of significantFiles) {
      try {
        const diff = execSync(
          `git diff ${baseCommit}..HEAD -- "${fileInfo.file}"`,
          {
            cwd: PROJECT_ROOT,
            encoding: 'utf-8',
            maxBuffer: 1024 * 1024 * 5, // 5MB buffer
          }
        );

        if (diff.trim()) {
          diffs.push({
            file: fileInfo.file,
            changes: fileInfo.totalChanges,
            diff: diff.slice(0, 5000), // Limit diff size to 5000 chars per file
          });
        }
      } catch (error) {
        // Skip files that error (e.g., binary files)
        continue;
      }
    }

    return {
      commits,
      changedFiles,
      significantFiles,
      diffs,
      categories: {
        backend: backendChanges,
        frontend: frontendChanges,
        tests: testChanges,
      },
      stats: {
        totalCommits: commits.length,
        totalFiles: changedFiles.length,
        significantFiles: significantFiles.length,
        backendFiles: backendChanges.length,
        frontendFiles: frontendChanges.length,
        diffsIncluded: diffs.length,
      },
    };
  } catch (error) {
    console.error('‚ùå Failed to get recent changes:', error.message);
    return {
      commits: [],
      changedFiles: [],
      significantFiles: [],
      diffs: [],
      categories: { backend: [], frontend: [], tests: [] },
      stats: { totalCommits: 0, totalFiles: 0, significantFiles: 0, backendFiles: 0, frontendFiles: 0, diffsIncluded: 0 },
    };
  }
}

/**
 * Load all architectural documentation
 *
 * @returns {Array<Object>} Architecture docs with content
 */
function loadArchitectureDocs() {
  const docs = [];

  for (const docPath of ARCHITECTURE_DOCS_PATHS) {
    const fullPath = path.join(PROJECT_ROOT, docPath);

    // Check if it's a directory or file
    const stats = fs.statSync(fullPath);

    if (stats.isDirectory()) {
      // Scan directory for markdown files
      const files = fs.readdirSync(fullPath);
      for (const file of files) {
        if (file.endsWith('.md')) {
          const filePath = path.join(fullPath, file);
          const content = fs.readFileSync(filePath, 'utf-8');
          docs.push({
            path: path.relative(PROJECT_ROOT, filePath),
            content,
            size: content.length,
            category: 'architecture',
          });
        }
      }
    } else if (stats.isFile()) {
      const content = fs.readFileSync(fullPath, 'utf-8');
      docs.push({
        path: path.relative(PROJECT_ROOT, fullPath),
        content,
        size: content.length,
        category: docPath.includes('architecture') ? 'architecture' : 'core',
      });
    }
  }

  return docs;
}

/**
 * Build context for architectural review
 *
 * @param {Object} changes - Recent significant changes
 * @param {Array<Object>} archDocs - Architecture documentation
 * @returns {string} Context prompt
 */
function buildArchitectureContext(changes, archDocs) {
  const context = [];

  context.push('# 9Boxer Architectural Review');
  context.push('\n## Your Role: Principal Software Engineer');
  context.push('You are a Principal Software Engineer with the wisdom and pragmatism of Martin Fowler.');
  context.push('You review code for architectural quality, detect drift from established patterns,');
  context.push('identify duplication, and ensure alignment with documented architectural principles.');

  context.push('\n## Project Context');
  context.push('**9Boxer** is a standalone desktop application (Electron + FastAPI + SQLite).');
  context.push('Multiple AI agents work independently on this codebase, creating risk of:');
  context.push('- **Duplication**: Same logic implemented in multiple places');
  context.push('- **Architectural Drift**: New code not following established patterns');
  context.push('- **Pattern Violations**: Breaking documented architectural conventions');
  context.push('- **Security Boundary Issues**: Violating the security model');
  context.push('- **Performance Regressions**: Changes impacting performance targets');

  context.push('\n## Recent Code Changes (Last 7 Days)');
  context.push(`\n**Summary:**`);
  context.push(`- Total commits: ${changes.stats.totalCommits}`);
  context.push(`- Significant files changed: ${changes.stats.significantFiles} (${changes.stats.totalFiles} total)`);
  context.push(`- Backend changes: ${changes.stats.backendFiles} files`);
  context.push(`- Frontend changes: ${changes.stats.frontendFiles} files`);
  context.push(`- Diffs included: ${changes.stats.diffsIncluded} files`);

  if (changes.commits.length > 0) {
    context.push('\n### Recent Commits:');
    changes.commits.slice(0, 20).forEach((commit) => {
      context.push(`- ${commit.hash}: ${commit.subject} (${commit.date})`);
    });
  }

  // Include actual code diffs for analysis
  if (changes.diffs.length > 0) {
    context.push('\n### Significant Code Changes (with diffs):');
    changes.diffs.forEach((diffInfo) => {
      context.push(`\n#### ${diffInfo.file} (+${diffInfo.changes} lines)`);
      context.push('```diff');
      context.push(diffInfo.diff);
      context.push('```');
    });
  }

  // Include architecture documentation
  context.push('\n## Established Architectural Patterns (FULL DOCS):');
  context.push('These are the documented architectural patterns and principles for this project.');
  context.push('Review code changes against these patterns to identify violations, drift, and missing documentation.\n');

  archDocs.forEach((doc) => {
    context.push(`\n### ${doc.path} (${doc.size} bytes)`);
    context.push('**FULL CONTENT:**');
    context.push(doc.content);
    context.push('\n---');
  });

  return context.join('\n');
}

/**
 * Analyze architecture with Claude API (principal-engineer persona)
 *
 * @param {string} context - Architectural review context
 * @returns {Promise<Object>} Review results
 */
async function analyzeArchitecture(context) {
  const apiKey = process.env.ANTHROPIC_API_KEY;

  if (!apiKey) {
    throw new Error('ANTHROPIC_API_KEY environment variable is required');
  }

  const anthropic = new Anthropic({ apiKey });

  const prompt = `${context}

## Your Task: Architectural Review

Analyze the recent code changes against established architectural patterns to identify issues.

**Focus Areas (prioritized):**

1. **Code Duplication** - Same or very similar logic in multiple files
   - Check for: Duplicate validation logic, duplicate API patterns, duplicate state management
   - Flag when: >70% similar code in 2+ locations

2. **Architectural Drift** - Code not following documented patterns
   - Check for: Error handling not matching ERROR_HANDLING.md patterns
   - Check for: Security violations not matching SECURITY_MODEL.md
   - Check for: Performance anti-patterns violating PERFORMANCE.md targets

3. **SOLID Principles Violations**
   - Single Responsibility: Functions/classes doing too much
   - Open/Closed: Modifications instead of extensions
   - Liskov Substitution: Subtypes breaking contracts
   - Interface Segregation: Clients forced to depend on unused methods
   - Dependency Inversion: Depending on concretions instead of abstractions

4. **Security Boundary Violations**
   - Renderer accessing Node.js APIs directly
   - IPC handlers without input validation
   - File paths using string concatenation instead of path.join()
   - Backend binding to 0.0.0.0 instead of 127.0.0.1

5. **Performance Regressions**
   - API responses >500ms
   - Grid rendering >2s
   - Missing database indexes on queries
   - N+1 query patterns

6. **Missing Architectural Documentation**
   - New patterns emerging in code that should be documented
   - Obsolete patterns still in docs but no longer used in code

**IMPORTANT CONSTRAINTS:**
- **High bar for issues**: Only flag SIGNIFICANT architectural concerns (not minor style issues)
- **Maximum 10 findings total** (prioritize critical and high priority)
- **Keep descriptions under 200 characters**
- **Keep remediation under 400 characters**
- **Provide specific code references** (file:line)

**For each finding, provide:**
1. **Type**: One of: duplication, drift, violation, security, performance, missing-docs
2. **Priority**: One of: critical, high, medium (DO NOT include low priority)
3. **Title**: Brief description (max 80 chars)
4. **Impact**: Why this matters (business/technical impact)
5. **Evidence**: Specific code locations and snippets
6. **Remediation**: Concrete refactoring steps with file references
7. **Effort**: Estimated effort (small: <2h, medium: 2-8h, large: >8h)

**For documentation update recommendations:**
1. **Doc**: Path to architecture doc to update
2. **Section**: Which section to update
3. **Recommendation**: What to add/change/remove
4. **Rationale**: Why this update is needed (e.g., "New pattern used in 5 commits")

**Return JSON format (ensure proper escaping):**
{
  "findings": [
    {
      "type": "duplication|drift|violation|security|performance|missing-docs",
      "priority": "critical|high|medium",
      "title": "Brief title",
      "impact": "Why this matters",
      "evidence": "Code locations and snippets",
      "remediation": "Specific refactoring steps",
      "effort": "small|medium|large"
    }
  ],
  "documentationUpdates": [
    {
      "doc": "internal-docs/architecture/ERROR_HANDLING.md",
      "section": "Pattern Catalog",
      "recommendation": "Add pattern for ...",
      "rationale": "Reason for update"
    }
  ],
  "summary": {
    "totalFindings": number,
    "byType": {
      "duplication": number,
      "drift": number,
      "violation": number,
      "security": number,
      "performance": number,
      "missing-docs": number
    },
    "byPriority": {
      "critical": number,
      "high": number,
      "medium": number
    },
    "criticalIssues": number,
    "docsToUpdate": number
  }
}

**JSON FORMATTING RULES:**
- Escape all quotes with backslash: \\"
- Escape all backslashes: \\\\
- No literal newlines in strings - use \\n
- Return ONLY the JSON object, no commentary before or after`;

  try {
    const message = await anthropic.messages.create({
      model: MODEL,
      max_tokens: MAX_TOKENS,
      messages: [
        {
          role: 'user',
          content: prompt,
        },
      ],
    });

    const content = message.content[0].text;

    // Extract JSON from response
    const jsonMatch = content.match(/```json\n([\s\S]*?)\n```/) || content.match(/({[\s\S]*})/);
    if (!jsonMatch) {
      console.error('‚ùå Failed to extract JSON from Claude response');
      console.error('Response preview:', content.slice(0, 500));
      throw new Error('Failed to extract JSON from Claude response');
    }

    const jsonText = jsonMatch[1];

    try {
      const result = JSON.parse(jsonText);
      return result;
    } catch (parseError) {
      console.error('‚ùå JSON parsing failed:', parseError.message);
      console.error('JSON excerpt around error position:');
      const errorPos = parseInt(parseError.message.match(/position (\d+)/)?.[1] || '0', 10);
      const start = Math.max(0, errorPos - 200);
      const end = Math.min(jsonText.length, errorPos + 200);
      console.error(jsonText.slice(start, end));
      throw new Error(`JSON parsing failed: ${parseError.message}`);
    }
  } catch (error) {
    console.error('‚ùå Architectural analysis failed:', error.message);
    throw error;
  }
}

/**
 * Create GitHub issues for architectural findings
 *
 * @param {Array<Object>} findings - Architectural findings
 * @param {string} reviewDate - Review date ISO string
 * @param {boolean} dryRun - If true, don't actually create issues
 * @returns {Promise<Array<Object>>} Created issues
 */
async function createArchitectureIssues(findings, reviewDate, dryRun = false) {
  const issues = [];

  // Only create issues for critical and high priority findings
  const issueableFindings = findings.filter((f) => f.priority === 'critical' || f.priority === 'high');

  if (issueableFindings.length === 0) {
    console.log('\n‚ú® No critical or high priority architectural issues found!');
    return issues;
  }

  console.log(`\nüìã Creating issues for ${issueableFindings.length} architectural findings...`);

  for (const finding of issueableFindings) {
    const weekOf = new Date(reviewDate).toISOString().split('T')[0];

    // Build issue body
    const sections = [];
    sections.push(ARCHITECTURE_ISSUE_PREAMBLE);
    sections.push('\n');

    const priorityIcon = finding.priority === 'critical' ? 'üî¥' : 'üü†';
    const typeIcons = {
      'duplication': 'üîÑ',
      'drift': 'üìê',
      'violation': '‚ö†Ô∏è',
      'security': 'üîí',
      'performance': '‚ö°',
      'missing-docs': 'üìù',
    };
    const typeIcon = typeIcons[finding.type] || '‚Ä¢';

    sections.push(`## ${priorityIcon} ${typeIcon} ${finding.title}`);
    sections.push(`\n**Type:** ${finding.type}`);
    sections.push(`**Priority:** ${finding.priority}`);
    sections.push(`**Effort:** ${finding.effort}`);
    sections.push(`**Detected:** ${reviewDate}`);

    sections.push('\n## Impact');
    sections.push(finding.impact);

    sections.push('\n## Evidence');
    sections.push(finding.evidence);

    sections.push('\n## Remediation Plan');
    sections.push(finding.remediation);

    sections.push('\n## Acceptance Criteria');
    sections.push('- [ ] Code duplication eliminated or abstraction created');
    sections.push('- [ ] Follows documented architectural patterns');
    sections.push('- [ ] Tests added or updated to verify fix');
    sections.push('- [ ] No new violations introduced');

    sections.push('\n---');
    sections.push(`\n*This issue was automatically generated by the AI architectural review system.*`);
    sections.push(`*Generated: ${reviewDate}*`);
    sections.push(`*Review Period: Week of ${weekOf}*`);

    const issueBody = sections.join('\n');

    // Determine labels
    const labels = ['architecture', 'technical-debt'];
    if (finding.priority === 'critical') {
      labels.push('priority: critical');
    } else if (finding.priority === 'high') {
      labels.push('priority: high');
    }

    // Add type-specific labels
    if (finding.type === 'duplication') labels.push('refactoring');
    if (finding.type === 'security') labels.push('security');
    if (finding.type === 'performance') labels.push('performance');

    const issueTitle = `${priorityIcon} ${typeIcon} ${finding.title}`;

    const issueData = {
      title: issueTitle,
      body: issueBody,
      labels,
    };

    if (dryRun) {
      console.log(`\nüìã [DRY RUN] Would create issue:`);
      console.log(`   Title: ${issueData.title}`);
      console.log(`   Labels: ${issueData.labels.join(', ')}`);
      issues.push({ ...issueData, number: 'DRY-RUN', finding });
      continue;
    }

    // Create actual issue
    let bodyFile = null;
    try {
      const tempDir = os.tmpdir();
      bodyFile = path.join(tempDir, `gh-issue-arch-${process.pid}-${Date.now()}.md`);

      fs.writeFileSync(bodyFile, issueData.body);

      const escapedTitle = issueData.title.replace(/"/g, '\\"').replace(/`/g, '\\`').replace(/\$/g, '\\$');
      const labelsArg = issueData.labels.map((l) => `--label "${l.replace(/"/g, '\\"')}"`).join(' ');

      const result = execSync(`gh issue create --title "${escapedTitle}" --body-file "${bodyFile}" ${labelsArg}`, {
        cwd: PROJECT_ROOT,
        encoding: 'utf-8',
      });

      const issueUrl = result.trim();
      const issueNumber = issueUrl.split('/').pop();

      console.log(`   ‚úÖ Created issue #${issueNumber}: ${finding.title}`);
      issues.push({ ...issueData, number: issueNumber, url: issueUrl, finding });
    } catch (error) {
      console.error(`   ‚ùå Failed to create issue: ${error.message}`);
    } finally {
      if (bodyFile && fs.existsSync(bodyFile)) {
        try {
          fs.unlinkSync(bodyFile);
        } catch (cleanupError) {
          console.warn(`‚ö†Ô∏è  Failed to clean up temp file: ${bodyFile}`);
        }
      }
    }
  }

  return issues;
}

/**
 * Save architectural review report to file
 *
 * @param {Object} report - Complete review report
 */
function saveReviewReport(report) {
  const reportPath = path.join(PROJECT_ROOT, 'arch-review-report.json');

  try {
    fs.writeFileSync(reportPath, JSON.stringify(report, null, 2));
    console.log(`\nüíæ Saved review report to: ${path.relative(PROJECT_ROOT, reportPath)}`);
  } catch (error) {
    console.error('‚ùå Failed to save review report:', error.message);
  }
}

/**
 * Print summary report
 *
 * @param {Object} report - Review report
 */
function printSummary(report) {
  console.log('\n' + '‚îÄ'.repeat(60));
  console.log('üèóÔ∏è  AI Architectural Review Summary');
  console.log('‚îÄ'.repeat(60));

  console.log(`\nüìÖ Review Period: Last ${report.metadata.daysScanned} days`);
  console.log(`üìä Code Changes Analyzed:`);
  console.log(`   - Commits: ${report.codeChanges.totalCommits}`);
  console.log(`   - Significant files: ${report.codeChanges.significantChanges}`);
  console.log(`   - Diffs analyzed: ${report.codeChanges.filesAnalyzed}`);

  console.log(`\nüîç Architectural Findings: ${report.summary.totalFindings}`);

  if (report.summary.totalFindings > 0) {
    console.log('   By Type:');
    Object.entries(report.summary.byType).forEach(([type, count]) => {
      if (count > 0) {
        const icons = {
          'duplication': 'üîÑ',
          'drift': 'üìê',
          'violation': '‚ö†Ô∏è',
          'security': 'üîí',
          'performance': '‚ö°',
          'missing-docs': 'üìù',
        };
        const icon = icons[type] || '‚Ä¢';
        console.log(`      ${icon} ${type}: ${count}`);
      }
    });

    console.log('   By Priority:');
    Object.entries(report.summary.byPriority).forEach(([priority, count]) => {
      if (count > 0) {
        const icon = priority === 'critical' ? 'üî¥' : priority === 'high' ? 'üü†' : 'üü°';
        console.log(`      ${icon} ${priority}: ${count}`);
      }
    });
  }

  if (report.findings.length > 0) {
    console.log('\nüìå Sample Findings:');
    report.findings.slice(0, 5).forEach((finding, index) => {
      const icon = finding.priority === 'critical' ? 'üî¥' : finding.priority === 'high' ? 'üü†' : 'üü°';
      console.log(`   ${index + 1}. ${icon} [${finding.type}] ${finding.title}`);
    });
    if (report.findings.length > 5) {
      console.log(`   ... and ${report.findings.length - 5} more`);
    }
  }

  if (report.documentationUpdates && report.documentationUpdates.length > 0) {
    console.log(`\nüìö Documentation Updates Recommended: ${report.documentationUpdates.length}`);
    report.documentationUpdates.slice(0, 3).forEach((update, index) => {
      console.log(`   ${index + 1}. ${update.doc}: ${update.recommendation}`);
    });
    if (report.documentationUpdates.length > 3) {
      console.log(`   ... and ${report.documentationUpdates.length - 3} more`);
    }
  }

  if (report.issues.length > 0) {
    console.log(`\nüé´ GitHub Issues Created: ${report.issues.length}`);
    report.issues.forEach((issue) => {
      console.log(`   - #${issue.number}: ${issue.finding.title}`);
      if (issue.url) {
        console.log(`     ${issue.url}`);
      }
    });
  } else if (report.metadata.dryRun) {
    console.log('\nüèÉ DRY RUN: No issues created');
  }

  console.log('\n' + '‚îÄ'.repeat(60));
}

/**
 * Main function
 */
async function main() {
  console.log('üèóÔ∏è  Starting AI-powered architectural review...\n');

  // Parse command line arguments
  const args = process.argv.slice(2);
  const daysArg = args.find((arg) => arg.startsWith('--days='));
  const days = daysArg ? parseInt(daysArg.split('=')[1], 10) : 7;
  const thresholdArg = args.find((arg) => arg.startsWith('--significance-threshold='));
  const significanceThreshold = thresholdArg ? parseInt(thresholdArg.split('=')[1], 10) : DEFAULT_SIGNIFICANCE_THRESHOLD;
  const dryRun = args.includes('--dry-run');

  if (dryRun) {
    console.log('üèÉ Running in DRY RUN mode (no issues will be created)\n');
  }

  // Validate environment
  if (!process.env.ANTHROPIC_API_KEY) {
    console.error('‚ùå ANTHROPIC_API_KEY environment variable is required');
    process.exit(1);
  }

  const reviewDate = new Date().toISOString();

  // Get significant code changes
  console.log(`üìÖ Analyzing significant changes from the last ${days} days...`);
  console.log(`   Significance threshold: ${significanceThreshold} lines changed`);
  const changes = getSignificantChanges(days, significanceThreshold);
  console.log(`   Found ${changes.stats.totalCommits} commits, ${changes.stats.significantFiles} significant files`);
  console.log(`   Including diffs for ${changes.stats.diffsIncluded} files`);

  // Load architecture documentation
  console.log('\nüìö Loading architectural documentation...');
  const archDocs = loadArchitectureDocs();
  console.log(`   Found ${archDocs.length} architecture documentation files`);

  // Build context
  console.log('\nüî® Building architectural review context...');
  const context = buildArchitectureContext(changes, archDocs);
  console.log(`   Context size: ${context.length} characters`);

  // Analyze with Claude
  console.log('\nü§ñ Analyzing architecture with principal-engineer agent...');
  const results = await analyzeArchitecture(context);
  console.log(`   Analysis complete: ${results.summary.totalFindings} finding(s)`);
  if (results.summary.criticalIssues > 0) {
    console.log(`   ‚ö†Ô∏è  ${results.summary.criticalIssues} CRITICAL issue(s) detected!`);
  }

  // Create GitHub issues
  console.log('\nüé´ Creating GitHub issues for significant findings...');
  const issues = await createArchitectureIssues(results.findings, reviewDate, dryRun);

  // Build complete report
  const report = {
    metadata: {
      generatedAt: reviewDate,
      generatedBy: 'ai-arch-review.js (principal-engineer)',
      daysScanned: days,
      significanceThreshold,
      dryRun,
    },
    codeChanges: {
      totalCommits: changes.stats.totalCommits,
      significantChanges: changes.stats.significantFiles,
      filesAnalyzed: changes.stats.diffsIncluded,
      backendFiles: changes.stats.backendFiles,
      frontendFiles: changes.stats.frontendFiles,
    },
    findings: results.findings,
    documentationUpdates: results.documentationUpdates || [],
    summary: {
      totalFindings: results.summary.totalFindings,
      byType: results.summary.byType,
      byPriority: results.summary.byPriority,
      criticalIssues: results.summary.criticalIssues,
      issuesCreated: issues.length,
      docsToUpdate: (results.documentationUpdates || []).length,
    },
    issues,
  };

  // Save report
  saveReviewReport(report);

  // Print summary
  printSummary(report);

  // Exit
  if (results.summary.totalFindings > 0) {
    console.log('\n‚ö†Ô∏è  Architectural issues detected!');
    console.log(`   Total findings: ${results.summary.totalFindings}`);
    console.log(`   Critical: ${results.summary.criticalIssues}`);
    console.log(`   GitHub issues created: ${issues.length}`);
    if (results.documentationUpdates && results.documentationUpdates.length > 0) {
      console.log(`   Documentation updates recommended: ${results.documentationUpdates.length}`);
    }
  } else {
    console.log('\n‚úÖ No significant architectural issues detected!');
  }
}

// Only run if executed directly
if (import.meta.url === `file://${process.argv[1]}`) {
  main().catch((error) => {
    console.error('\n‚ùå Architectural review failed:', error.message);
    process.exit(1);
  });
}

// Export functions for testing
export {
  getSignificantChanges,
  loadArchitectureDocs,
  buildArchitectureContext,
  analyzeArchitecture,
  createArchitectureIssues,
};
