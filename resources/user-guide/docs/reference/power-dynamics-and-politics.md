# Power Dynamics and Politics in Calibration

**When to read this:** You're facilitating calibration sessions or want to understand the unspoken dynamics at play

**Time to read:** 8 minutes

**What you'll learn:** How power shapes calibration conversations and what you can do about it

---

## Introduction

Let's talk about the thing nobody mentions in calibration guides: power.

Calibration isn't a neutral exercise. When you put managers in a room to discuss their people, you're creating a situation where influence, credibility, and organizational politics come into play. Some managers' ratings get challenged. Others don't. Someone's opinion carries more weight. Someone else stays quiet.

This document acknowledges those dynamics openly. We're not going to pretend calibration happens in a vacuum where everyone's voice counts equally and decisions are purely data-driven. That's not reality.

**This guide is for:**

- **Facilitators** who need to create fair conversations despite power imbalances
- **HR leaders** who want to prevent political calibration
- **Managers** who want to understand what's really happening in the room
- **Anyone** who's felt calibration turn into theater instead of truth-telling

If you've ever watched a rating go unchallenged because of who proposed it, this is for you.

---

## The Dual Purpose Nobody Talks About

Here's what calibration actually does.

### It Assesses Employees AND Managers Simultaneously

When you explain your ratings to peers, you're not just talking about your employees. You're demonstrating your own judgment.

Everyone in the room is evaluating:

- Do you understand what good performance looks like?
- Can you articulate why someone is a High Performer?
- Are you over-rating (too lenient) or under-rating (too harsh)?
- Do you advocate appropriately for your people or play politics?

Managers know this. They know calibration is partially about assessing them. When documentation pretends it's only about the employees, it creates cognitive dissonance that breeds distrust.

### Managers Are Being Evaluated on Their Judgment

Think about what happens when a manager consistently:

- Rates everyone High (can't differentiate or avoids hard conversations)
- Rates everyone Medium (playing it safe, unclear standards)
- Can't explain their ratings (working from gut feel, not evidence)
- Changes their mind immediately when challenged (weak convictions, people-pleasing)

Other managers notice. HR notices. Leadership notices.

Your credibility as a manager grows or shrinks based on how you calibrate. Managers who demonstrate good judgment gain influence over time. Those who don't, lose it.

### Transparency Creates Discomfort by Design

Calibration makes private ratings visible to peers. This is uncomfortable by design.

When you rated someone High in isolation, it felt justified. When you compare that person to other High Performers across the organization, you might realize your bar is different. That's the point.

The discomfort is the work. If calibration feels comfortable and easy, you're probably not actually calibrating.

### Why Acknowledging This Builds Trust

When you pretend calibration is only about employees, managers feel manipulated. They know they're being evaluated but can't acknowledge it openly.

When you name it directly, you create permission to be uncertain:

> "Calibration serves a dual purpose: aligning on employee ratings AND developing manager judgment. When you explain your ratings to peers, you're demonstrating your understanding of what excellence looks like at each level. This is healthy—it helps everyone get better at recognizing and developing talent. Approach calibration as an opportunity to sharpen your own judgment, not just defend your ratings."

This framing makes it okay to change your mind. It makes it okay to say "I might have gotten this wrong." It transforms calibration from a test you pass or fail into a learning opportunity.

### The Power Dynamic: Good Judgment Gains Influence

Over time, managers who consistently demonstrate good judgment gain influence in calibration.

When Sarah shares her assessment, people listen because she's been right before. Her ratings tend to align with actual outcomes. She can articulate clear evidence. She changes her mind when presented with new information.

When Marcus shares his, people are skeptical because he's consistently over-rated. His High Performers often underdeliver. He struggles to explain why someone deserves their rating. He digs in when challenged.

This credibility redistribution is healthy organizational development. It's not a gotcha game—it's how teams learn what good judgment looks like.

---

## When Power Protects Ratings

The hardest calibration moments happen when power dynamics prevent honest conversations.

### The "Protected Pet" Scenario

**The situation:** A senior leader has a direct report who is clearly underperforming. Maybe they're friends outside work. Maybe the leader hired them and doesn't want to admit the mistake. Maybe they're someone's relative.

Everyone in the room knows this person is over-rated. Everyone sees the inflated rating on the screen. And everyone stays quiet.

**What happens when this goes unaddressed:**

The room learns that calibration is theater. If certain people are immune from scrutiny, the whole exercise loses credibility. Trust in the process evaporates.

Other managers think: "Why should I honestly rate my people when protected employees get special treatment?" The next calibration, more people play politics.

**How to name it gently as a facilitator:**

You don't need to create confrontation. You need to create conversation.

Try: "I notice we moved quickly past this one. Let's compare this person to the other High Performers we've discussed—how does the evidence stack up?"

Or: "Walk me through what makes this person a High Performer. What specific outcomes or behaviors put them in this category?"

Or: "I want to make sure we're using the same bar across the organization. If this person is High Performance, what does that tell us about how we've rated others?"

Your job as facilitator is to make the conversation happen, even when it's uncomfortable. Not to force a specific outcome, but to ensure the rating gets the same scrutiny as everyone else's.

### Making Conversation Happen Despite Discomfort

Sometimes the room goes quiet because no one wants to be the first to challenge a senior leader's rating.

**What this looks like:**

- Facilitator: "Any thoughts on this placement?"
- Room: [silence]
- Facilitator: "Okay, moving on..."
- Everyone thinks: "We all know that rating is wrong, but no one wants to say it."

**What to do instead:**

Name the silence: "I notice we got quiet on that one. Is there something we're not saying?"

Or create comparison: "Let's look at the other High Performers we've discussed. How does this person's impact compare to Sarah's High Performer who shipped the Q4 feature?"

Or model uncertainty yourself: "I'm curious about this placement. When I look at the evidence compared to others in this category, I see a gap. What am I missing?"

Sometimes people need permission to disagree. Give it to them.

### What to Do When Senior Leaders Dominate

In some calibration sessions, senior leaders' opinions end every discussion. They share their view first. Everyone else falls in line. Conversation stops.

**This kills calibration.** You're not calibrating—you're rubber-stamping executive opinions.

**Tactics for facilitators:**

**1. Senior leaders go last**

Reverse the usual speaking order. Invite the most junior people to share their perspective first. Let senior leaders hear other views before offering theirs.

This is hard. Senior leaders are used to speaking first. Frame it explicitly:

> "For this session, I'm going to ask our senior leaders to hold their perspectives until everyone else has shared. We want to hear all voices before seniority shapes the conversation."

**2. Create space for dissent**

After a senior leader shares their view, don't move on immediately. Create explicit space:

> "That's one perspective. Who sees it differently?"

Or: "Before we move forward, I want to hear if anyone has a different read on this person."

**3. Model disagreement yourself**

If you're the facilitator, disagree with a senior leader at least once per session (when you genuinely disagree, not performatively).

This shows the room that disagreement is allowed:

> "I hear your perspective, and I see it differently. When I look at this person's outcomes compared to others at their level, I see a gap in [specific area]. Help me understand what I'm missing."

When you model productive disagreement, you give others permission to do the same.

---

## Redistributing Credibility

Calibration naturally redistributes credibility over time. This is healthy, not harmful.

### Managers Who Demonstrate Good Judgment Gain Influence

Over multiple calibration sessions, patterns emerge.

Some managers consistently:

- Articulate clear, evidence-based rationales
- Rate in alignment with organizational standards
- Change their ratings when presented with new information
- Predict outcomes accurately (their High Performers actually deliver)

These managers gain influence. Not because of their title, but because their judgment proves reliable.

When they say "I think this person is a High Performer," people listen. Their track record has earned credibility.

### Those Who Can't Articulate Ratings Lose Credibility

Other managers consistently:

- Rate based on gut feel without evidence
- Over-rate their entire team
- Can't explain why someone deserves their rating
- Refuse to adjust ratings despite evidence

These managers lose credibility. Not because anyone's trying to punish them, but because their judgment hasn't proven reliable.

When they say "This person is a Star," people are skeptical. The room needs more evidence because past ratings haven't aligned with outcomes.

### This Is Healthy Organizational Development

This credibility redistribution isn't a gotcha game. It's how organizations learn what good judgment looks like.

When you track which managers' ratings predict actual outcomes—promotions, successful projects, retention of top talent—you identify who understands performance assessment well. Those managers become teachers for others.

When you identify managers whose ratings consistently misalign with outcomes, you create coaching opportunities. Maybe they need clearer performance definitions. Maybe they're avoiding difficult conversations. Maybe their team genuinely is that strong (or weak) compared to others.

The data from calibration patterns helps develop managers, not just employees.

### Track Which Managers' Ratings Predict Actual Outcomes

Smart organizations track calibration patterns over time:

- Do managers' High Performers actually get promoted?
- Do their Low Performers improve or exit?
- Do their ratings align with other performance signals (project outcomes, peer feedback, business results)?

This isn't about catching managers being wrong. It's about understanding whose judgment is most predictive and learning from them.

If Manager A's High Performer ratings correlate strongly with future promotions and impact, what can other managers learn from how Manager A assesses performance?

If Manager B consistently rates people higher than outcomes justify, what support does Manager B need to calibrate better?

### Learn from Managers Who Assess Well

The best way to improve organizational calibration is to learn from managers who do it well.

After calibration, identify managers whose ratings:

- Aligned well with peer consensus
- Were supported by clear evidence
- Predicted outcomes accurately in past cycles

Ask them to share their approach:

- How do they think about the difference between Medium and High Performance?
- What evidence do they look for?
- How do they separate likability from performance?
- How do they handle borderline cases?

Treating calibration as manager development—not just employee assessment—transforms it from a necessary evil into organizational learning.

---

## Avoiding Political Calibration

Calibration can become political. Here's how to prevent it.

### Data First, Discussion Second

**The problem:** When you start calibration by asking managers to advocate for their people, you create a negotiation. The most persuasive speakers win, not the most accurate assessments.

**The solution:** Look at the distribution data before anyone advocates for anyone.

**How this works:**

1. Open the Statistics tab: Show the overall distribution before reviewing individuals
2. Identify patterns: "We have 35% rated High Performance—that's unusually high. Let's understand why."
3. Look at manager-level data: "Manager A rated 8 of 10 people High. Manager B rated 1 of 12 High. Let's understand these differences."
4. Then review individuals: Now when you discuss specific employees, you have context

Starting with data depersonalizes the conversation. You're not attacking Manager A for leniency—you're observing a pattern that needs discussion.

### Level-Based Review Makes Patterns Visible

**The problem:** When you review manager-by-manager, it's hard to spot inconsistencies. You review Manager A's team, discuss their ratings, move to Manager B's team, discuss those. You never see all Senior Engineers side by side.

**The solution:** Review all employees at the same level together, across managers.

**Why this prevents politics:**

When you filter to show all Senior Engineers regardless of manager, patterns become obvious:

- Manager A rated all their Senior Engineers High
- Manager B rated all theirs Medium
- Manager C has a mix

Now you can ask: "Are Manager A's Senior Engineers genuinely stronger, or is the bar different?" This is much harder to see when reviewing manager-by-manager.

Level-based review makes it harder to play politics because everyone's ratings are directly comparable.

### Rotate Facilitators

**The problem:** When the same person facilitates every calibration, they can shape outcomes through selective questioning, allowing certain discussions to end early, or applying scrutiny unevenly.

**The solution:** Rotate who facilitates calibration sessions.

**Why this helps:**

Different facilitators notice different patterns. They ask different questions. They push back on different things.

If Manager A's ratings sailed through when Facilitator X ran the session but get challenged when Facilitator Y facilitates, that inconsistency becomes visible.

Rotation also prevents any single person from accumulating too much influence over the process.

### Document the Reasoning

**The problem:** Calibration decisions made in the moment become folklore. Three months later, no one remembers why Alex was moved from High to Medium. Six months later, a new manager asks why Sarah is rated Low and no one can explain.

**The solution:** Write down the rationale for every rating change.

**What this looks like:**

When you move someone's rating during calibration, add a note:

- "Moved to High based on Q4 delivery of the integration project ahead of schedule and demonstrated leadership in mentoring two junior engineers."
- "Moved to Medium—when compared to other High Performers at this level, this person hasn't demonstrated the strategic thinking we expect."
- "Kept at Low—continuing performance issues in code quality despite coaching; needs performance improvement plan."

**Why this prevents politics:**

Documentation creates accountability. When someone wants to inflate a rating for political reasons, asking "What will we write as the rationale?" forces clarity.

Written rationales also prevent the same arguments from recurring. You've already discussed this placement. The reasoning is documented. If circumstances change, fine—but you're not relitigating the same decision based on politics.

### Check Outcomes Over Time

**The problem:** If calibration decisions never connect to real outcomes, the process becomes disconnected from reality. You can play politics indefinitely because there's no feedback loop.

**The solution:** Track whether calibration ratings predict actual results.

**What to track:**

- **Promotions:** Do your High Performers actually get promoted? If not, are they really High Performers?
- **Exits:** Do your Low Performers improve or leave? If they stay and nothing changes, what's the point of the rating?
- **Project outcomes:** Do the people you rated as Stars actually deliver star performance on projects?
- **Development success:** Do people you rated as High Potential actually grow when given opportunities?

**Why this prevents politics:**

When you close the feedback loop, political ratings become visible. If someone keeps protecting their underperforming direct report and that person keeps underdelivering, the pattern becomes undeniable.

If a manager keeps fighting for High ratings and those people consistently fail to deliver high-impact work, their credibility erodes.

Outcome tracking makes calibration self-correcting over time.

### Signs That Calibration Is Becoming Political

Watch for these patterns:

**Same people protected every time**

If certain employees' ratings never get challenged regardless of evidence, calibration is political. Favorites are being protected.

**Ratings follow org chart rather than performance**

If senior leaders' direct reports are always rated higher than others at the same level with similar performance, the process is being influenced by proximity to power.

**Facilitator always "agrees" with senior leaders**

If the facilitator consistently sides with whoever has the most organizational authority, they're not facilitating—they're enabling political calibration.

**Managers negotiate trades**

"I'll move my person from High to Medium if you move yours"—this is pure politics. You're not assessing performance; you're horse-trading to preserve everyone's ratings.

**Evidence doesn't matter**

If specific examples and data don't change outcomes, and decisions are based on relationships or seniority, calibration has become political theater.

**No one's rating ever improves or worsens**

If the distribution never changes year over year despite people developing or declining, you're not calibrating—you're rubber-stamping.

---

## What This Means for You

If you're reading this, you probably care about making calibration fair.

**If you're a facilitator:**

Your job is to create the conditions where honest conversation can happen despite power dynamics. Name what's happening in the room. Create space for dissent. Make senior leaders go last. Ask for evidence. Document reasoning.

You can't eliminate power dynamics. But you can prevent them from making calibration purely political.

**If you're a manager:**

Approach calibration as a chance to demonstrate and sharpen your judgment, not defend your territory. Come with evidence. Stay curious about other perspectives. Build credibility over time by making good calls.

**If you're an HR leader:**

Design calibration processes that make politics visible and costly. Use level-based reviews. Track outcomes. Rotate facilitators. Document decisions.

The goal isn't perfect objectivity—that's impossible. The goal is making calibration honest enough that it actually improves decision-making instead of protecting favorites.

---

## Related Resources

- Back to [Complete Calibration Guide](../best-practices.md)
- Back to [New to 9-Box? Start Here](../new-to-9box.md)
- Related: [Creating Psychological Safety](psychological-safety.md)
- Related: [Difficult Scenarios in Calibration](difficult-scenarios.md)
- Related: [Post-Calibration Conversations](post-calibration-conversations.md)

---

**Remember:** Power dynamics exist. Acknowledging them openly makes calibration more honest, not less.
